{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-12-12T17:49:17.286116Z",
     "iopub.status.busy": "2024-12-12T17:49:17.285816Z",
     "iopub.status.idle": "2024-12-12T17:49:32.127092Z",
     "shell.execute_reply": "2024-12-12T17:49:32.126173Z",
     "shell.execute_reply.started": "2024-12-12T17:49:17.286080Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (2004, 64, 128, 1)\n",
      "X_val shape: (501, 64, 128, 1)\n",
      "y_train shape: (2004, 64, 128, 1)\n",
      "y_val shape: (501, 64, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import data \n",
    "data = np.load('/kaggle/input/homework2cleaned/mars_no_aliens.npz')\n",
    "training_set = data[\"training_set\"]\n",
    "\n",
    "X_train = training_set[:, 0]\n",
    "y_train = training_set[:, 1]\n",
    "\n",
    "X_test = data[\"test_set\"]\n",
    "\n",
    "#add dimention \n",
    "if X_train.ndim == 3:\n",
    "    X_train = np.expand_dims(X_train, axis=-1)  # 转换为 (samples, height, width, 1)\n",
    "if y_train.ndim == 3:\n",
    "    y_train = np.expand_dims(y_train, axis=-1)  # 转换为 (samples, height, width, 1)\n",
    "\n",
    "# split train and validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "#y_val = tf.keras.utils.to_categorical(y_val, num_classes=5)\n",
    "\n",
    "\n",
    "print(f\"X_train shape: {X_train.shape}\")  # should be (2092, 64, 128, 1)\n",
    "print(f\"X_val shape: {X_val.shape}\")      # should be (523, 64, 128, 1)\n",
    "print(f\"y_train shape: {y_train.shape}\")  # should be (2092, 64, 128, 1)\n",
    "print(f\"y_val shape: {y_val.shape}\")      # should be (523, 64, 128, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T17:51:03.631414Z",
     "iopub.status.busy": "2024-12-12T17:51:03.631045Z",
     "iopub.status.idle": "2024-12-12T17:51:03.642565Z",
     "shell.execute_reply": "2024-12-12T17:51:03.641640Z",
     "shell.execute_reply.started": "2024-12-12T17:51:03.631382Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import scipy.ndimage\n",
    "\n",
    "# image size and batch size\n",
    "IMAGE_SIZE = (64, 128)\n",
    "BATCH_SIZE = 32\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "def preprocess_image_and_mask(image, mask):\n",
    "    image = tf.cast(image, tf.float32)/255.0\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    image = tf.image.grayscale_to_rgb(image)\n",
    "    # mask = tf.one_hot(tf.cast(mask, tf.int32), depth=5)\n",
    "    return image, mask\n",
    "\n",
    "def elastic_transform(image, mask, alpha, sigma, random_state=None):\n",
    "    \n",
    "    if random_state is None:\n",
    "        random_state = np.random.RandomState(None)\n",
    "\n",
    "    shape = image.shape[:2]  # Only height and width are needed\n",
    "\n",
    "    # Generate random displacement fields\n",
    "    dx = random_state.uniform(-1, 1, size=shape) * alpha\n",
    "    dy = random_state.uniform(-1, 1, size=shape) * alpha\n",
    "\n",
    "    # Smooth the displacement fields using Gaussian filter\n",
    "    dx = scipy.ndimage.gaussian_filter(dx, sigma=sigma, mode=\"constant\", cval=0)\n",
    "    dy = scipy.ndimage.gaussian_filter(dy, sigma=sigma, mode=\"constant\", cval=0)\n",
    "\n",
    "    # Create coordinate grid\n",
    "    x, y = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]))\n",
    "    indices = np.stack([(y + dy).flatten(), (x + dx).flatten()])\n",
    "\n",
    "    # Apply transformation for each channel independently\n",
    "    transformed_image = np.zeros_like(image)\n",
    "    for i in range(image.shape[2]):  # Iterate over channels\n",
    "        transformed_image[..., i] = scipy.ndimage.map_coordinates(\n",
    "            image[..., i], indices, order=1, mode='reflect'\n",
    "        ).reshape(shape)\n",
    "\n",
    "    # Apply transformation for mask\n",
    "    transformed_mask = scipy.ndimage.map_coordinates(\n",
    "        mask[..., 0], indices, order=1, mode='reflect'\n",
    "    ).reshape(shape + (1,))\n",
    "\n",
    "    return transformed_image, transformed_mask\n",
    "\n",
    "def elastic_transform_wrapper(image, mask, alpha=34, sigma=4):\n",
    "    def numpy_transform(image, mask):\n",
    "        image = image.numpy()\n",
    "        mask = mask.numpy()\n",
    "        transformed_image, transformed_mask = elastic_transform(image, mask, alpha, sigma)\n",
    "\n",
    "        # Ensure mask shape has a single channel\n",
    "        if transformed_mask.ndim == 2:  \n",
    "            transformed_mask = np.expand_dims(transformed_mask, axis=-1)\n",
    "        return transformed_image, transformed_mask\n",
    "\n",
    "    transformed_image, transformed_mask = tf.py_function(\n",
    "        func=numpy_transform,\n",
    "        inp=[image, mask],\n",
    "        Tout=[tf.float32, tf.float32]  \n",
    "    )\n",
    "\n",
    "    transformed_image.set_shape(image.shape)\n",
    "    transformed_mask.set_shape(mask.shape)\n",
    "    return transformed_image, transformed_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T17:51:09.286078Z",
     "iopub.status.busy": "2024-12-12T17:51:09.285260Z",
     "iopub.status.idle": "2024-12-12T17:51:09.673934Z",
     "shell.execute_reply": "2024-12-12T17:51:09.673200Z",
     "shell.execute_reply.started": "2024-12-12T17:51:09.286044Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def extract_regions_with_mask(image, mask, target_label=4):\n",
    "    \"\"\"\n",
    "    extract `mask == 4` region\n",
    "    \"\"\"\n",
    "    regions = []\n",
    "    mask_positions = np.where(mask == target_label)  \n",
    "    if len(mask_positions[0]) > 0:\n",
    "        x_min, x_max = mask_positions[0].min(), mask_positions[0].max()\n",
    "        y_min, y_max = mask_positions[1].min(), mask_positions[1].max()\n",
    "        cropped_region = image[x_min:x_max+1, y_min:y_max+1]\n",
    "        cropped_mask = mask[x_min:x_max+1, y_min:y_max+1]\n",
    "        regions.append((cropped_region, cropped_mask))\n",
    "    return regions\n",
    "\n",
    "def insert_region(image, mask, region, region_mask):\n",
    "    \"\"\"\n",
    "    random insert\n",
    "    \"\"\"\n",
    "    region_h, region_w, _ = region.shape\n",
    "    img_h, img_w, _ = image.shape\n",
    "\n",
    "    max_x = img_h - region_h\n",
    "    max_y = img_w - region_w\n",
    "    \n",
    "    if max_x < 0 or max_y < 0:\n",
    "        return image, mask\n",
    "    \n",
    "    start_x = random.randint(0, max_x)\n",
    "    start_y = random.randint(0, max_y)\n",
    "    \n",
    "    updated_image = image.copy()\n",
    "    updated_image[start_x:start_x+region_h, start_y:start_y+region_w] = region\n",
    "    \n",
    "    updated_mask = mask.copy()\n",
    "    updated_mask[start_x:start_x+region_h, start_y:start_y+region_w] = region_mask\n",
    "    \n",
    "    return updated_image, updated_mask\n",
    "\n",
    "def augment_with_fixed_regions(X_train, y_train, target_label=4, num_regions=1):\n",
    "    \"\"\"\n",
    "    ensuse every pic contains `mask=4` region，and maintian the shape\n",
    "    \"\"\"\n",
    "    images_with_target = [(img, msk) for img, msk in zip(X_train, y_train) if np.any(msk == target_label)]\n",
    "    regions = []\n",
    "    for image, mask in images_with_target:\n",
    "        regions.extend(extract_regions_with_mask(image, mask, target_label))\n",
    "    \n",
    "    if not regions:\n",
    "        print(\"No regions with mask=4 found.\")\n",
    "        return X_train, y_train\n",
    "    \n",
    "    augmented_X_train = []\n",
    "    augmented_y_train = []\n",
    "    \n",
    "    for image, mask in zip(X_train, y_train):\n",
    "        augmented_image = image.copy()\n",
    "        augmented_mask = mask.copy()\n",
    "        \n",
    "        for _ in range(num_regions):\n",
    "            region, region_mask = random.choice(regions)  \n",
    "            augmented_image, augmented_mask = insert_region(augmented_image, augmented_mask, region, region_mask)\n",
    "        \n",
    "        augmented_X_train.append(augmented_image)\n",
    "        augmented_y_train.append(augmented_mask)\n",
    "    \n",
    "    return np.array(augmented_X_train), np.array(augmented_y_train)\n",
    "\n",
    "\n",
    "X_train_augmented, y_train_augmented = augment_with_fixed_regions(X_train, y_train, target_label=4, num_regions=1)\n",
    "#y_train_one_hot = tf.keras.utils.to_categorical(y_train_augmented, num_classes=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T17:51:14.422569Z",
     "iopub.status.busy": "2024-12-12T17:51:14.421773Z",
     "iopub.status.idle": "2024-12-12T17:51:16.015436Z",
     "shell.execute_reply": "2024-12-12T17:51:16.014740Z",
     "shell.execute_reply.started": "2024-12-12T17:51:14.422533Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# train dataset\n",
    "original_train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "original_train_dataset = (\n",
    "    original_train_dataset\n",
    "    .map(preprocess_image_and_mask, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    #.map(elastic_transform_wrapper,num_parallel_calls = tf.data.AUTOTUNE)\n",
    "    .shuffle(10 * BATCH_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "# validation dataset\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "val_dataset = (\n",
    "    val_dataset\n",
    "    .map(preprocess_image_and_mask, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.AUTOTUNE)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T17:51:18.366859Z",
     "iopub.status.busy": "2024-12-12T17:51:18.366526Z",
     "iopub.status.idle": "2024-12-12T17:51:18.872569Z",
     "shell.execute_reply": "2024-12-12T17:51:18.871880Z",
     "shell.execute_reply.started": "2024-12-12T17:51:18.366830Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import other libraries\n",
    "import os\n",
    "import math\n",
    "from PIL import Image\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "NUM_CLASSES =5\n",
    "seed = 42\n",
    "\n",
    "# Import TensorFlow and Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as tfk\n",
    "from tensorflow.keras import layers as tfkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T18:49:26.910477Z",
     "iopub.status.busy": "2024-12-12T18:49:26.909712Z",
     "iopub.status.idle": "2024-12-12T18:49:26.917546Z",
     "shell.execute_reply": "2024-12-12T18:49:26.916661Z",
     "shell.execute_reply.started": "2024-12-12T18:49:26.910444Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def attention_weighted_fusion(skip_connection, upsampled_features, filters, name):\n",
    "    # Adjust skip connection to match upsampled features dimensions\n",
    "    skip_connection = conv1x1(skip_connection, filters, name=name + \"_adjust_skip\")\n",
    "    upsampled_features = conv1x1(upsampled_features, filters, name=name + \"_adjust_upsampled\")\n",
    "\n",
    "    # Ensure spatial dimensions match\n",
    "    skip_shape = tf.shape(skip_connection)\n",
    "    upsample_shape = tf.shape(upsampled_features)\n",
    "\n",
    "    if skip_shape[1] != upsample_shape[1] or skip_shape[2] != upsample_shape[2]:\n",
    "        upsampled_features = tf.image.resize(upsampled_features, [skip_shape[1], skip_shape[2]])\n",
    "\n",
    "    # Attention mechanism\n",
    "    gate = Conv2D(filters, kernel_size=1, padding='same', kernel_initializer=initializer, name=name + '_gate_conv')(upsampled_features)\n",
    "    attention = Add(name=name + '_add')([skip_connection, gate])\n",
    "    attention = tf.keras.layers.Activation('relu', name=name + '_activation')(attention)\n",
    "    attention_weights = Conv2D(1, kernel_size=1, padding='same', activation='sigmoid', kernel_initializer=initializer, name=name + '_attention_weights')(attention)\n",
    "\n",
    "    # Apply attention weights\n",
    "    weighted_skip = Multiply(name=name + \"_weighted_skip\")([attention_weights, skip_connection])\n",
    "    weighted_upsampled = Multiply(name=name + \"_weighted_upsampled\")([(1 - attention_weights), upsampled_features])\n",
    "\n",
    "    # Combine skip and upsampled features\n",
    "    fused_features = Add(name=name + \"_fused_features\")([weighted_skip, weighted_upsampled])\n",
    "    return fused_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T17:51:21.657851Z",
     "iopub.status.busy": "2024-12-12T17:51:21.657308Z",
     "iopub.status.idle": "2024-12-12T17:51:21.680666Z",
     "shell.execute_reply": "2024-12-12T17:51:21.680018Z",
     "shell.execute_reply.started": "2024-12-12T17:51:21.657819Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, LayerNormalization, BatchNormalization, Add, Multiply, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "initializer = tf.keras.initializers.HeNormal()\n",
    "\n",
    "# 1x1 convolution for dimension adjustment\n",
    "def conv1x1(inputs, filters, name):\n",
    "    return Conv2D(filters, kernel_size=1, padding='same', kernel_initializer=initializer, name=name)(inputs)\n",
    "\n",
    "# U-Net block with Batch Normalization\n",
    "def unet_block_with_bn(inputs, filters, name):\n",
    "    x = Conv2D(filters, kernel_size=3, padding='same', kernel_initializer=initializer, name=name + 'conv1')(inputs)\n",
    "    x = BatchNormalization(name=name + 'bn1')(x)\n",
    "    x = tf.keras.layers.ReLU(name=name + 'relu1')(x)\n",
    "\n",
    "    x = Conv2D(filters, kernel_size=3, padding='same', kernel_initializer=initializer, name=name + 'conv2')(x)\n",
    "    x = BatchNormalization(name=name + 'bn2')(x)\n",
    "    x = tf.keras.layers.ReLU(name=name + 'relu2')(x)\n",
    "\n",
    "    x = Conv2D(filters, kernel_size=3, padding='same', kernel_initializer=initializer, name=name + 'conv3')(x)\n",
    "    x = BatchNormalization(name=name + 'bn3')(x)\n",
    "    x = tf.keras.layers.ReLU(name=name + 'relu3')(x)\n",
    "    return x\n",
    "\n",
    "# U-Net block with Layer Normalization\n",
    "def unet_block_with_ln(inputs, filters, name):\n",
    "    x = Conv2D(filters, kernel_size=3, padding='same', kernel_initializer=initializer, name=name + 'conv1')(inputs)\n",
    "    x = LayerNormalization(name=name + 'ln1')(x)\n",
    "    x = tf.keras.layers.ReLU(name=name + 'relu1')(x)\n",
    "\n",
    "    x = Conv2D(filters, kernel_size=3, padding='same', kernel_initializer=initializer, name=name + 'conv2')(x)\n",
    "    x = LayerNormalization(name=name + 'ln2')(x)\n",
    "    x = tf.keras.layers.ReLU(name=name + 'relu2')(x)\n",
    "\n",
    "    x = Conv2D(filters, kernel_size=3, padding='same', kernel_initializer=initializer, name=name + 'conv3')(x)\n",
    "    x = LayerNormalization(name=name + 'ln3')(x)\n",
    "    x = tf.keras.layers.ReLU(name=name + 'relu3')(x)\n",
    "    return x\n",
    "\n",
    "# SE Attention Module\n",
    "def se_block(inputs, reduction=16, name=\"se_block\"):\n",
    "    filters = inputs.shape[-1]\n",
    "    se = tf.keras.layers.GlobalAveragePooling2D(name=name + '_gap')(inputs)\n",
    "    se = tf.keras.layers.LayerNormalization(name=name + '_ln1')(se)\n",
    "    se = tf.keras.layers.Dense(filters // reduction, activation='relu', name=name + '_fc1')(se)\n",
    "    se = tf.keras.layers.LayerNormalization(name=name + '_ln2')(se)\n",
    "    se = tf.keras.layers.Dense(filters, activation='sigmoid', name=name + '_fc2')(se)\n",
    "    se = tf.keras.layers.Reshape([1, 1, filters], name=name + '_reshape')(se)\n",
    "    se = tf.keras.layers.Multiply(name=name + '_scale')([inputs, se])\n",
    "    return se\n",
    "\n",
    "# Bottleneck with SE Attention\n",
    "def bottleneck_with_attention(inputs, filters, name):\n",
    "    x = unet_block_with_ln(inputs, filters, name=name + \"_unet_block\")\n",
    "    x = se_block(x, name=name + \"_se_block\")\n",
    "    return x\n",
    "\n",
    "# Dynamic Weighted Fusion\n",
    "def dynamic_weighted_fusion(skip_connection, upsampled_features, name):\n",
    "    skip_connection = conv1x1(skip_connection, upsampled_features.shape[-1], name=name + \"_adjust_skip\")\n",
    "    skip_weight = tf.Variable(initial_value=0.5, trainable=True, name=name + \"_skip_weight\")\n",
    "    fused_features = Add(name=name + \"_fused_features\")([\n",
    "        skip_weight * skip_connection,\n",
    "        (1 - skip_weight) * upsampled_features\n",
    "    ])\n",
    "    return fused_features\n",
    "\n",
    "# Attention Gate\n",
    "def attention_gate(encoder_features, decoder_features, filters, name):\n",
    "    gate = Conv2D(filters, kernel_size=1, padding='same', kernel_initializer=initializer, name=name + '_gate_conv')(decoder_features)\n",
    "    skip = Conv2D(filters, kernel_size=1, padding='same', kernel_initializer=initializer, name=name + '_skip_conv')(encoder_features)\n",
    "    combined = Add(name=name + '_add')([gate, skip])\n",
    "    combined = tf.keras.layers.Activation('relu', name=name + '_activation')(combined)\n",
    "    combined = Conv2D(1, kernel_size=1, padding='same', activation='sigmoid', kernel_initializer=initializer, name=name + '_attention_weights')(combined)\n",
    "    attention = Multiply(name=name + '_attention_multiply')([encoder_features, combined])\n",
    "    return attention\n",
    "\n",
    "\"\"\"\n",
    "# Full U-Net Model\n",
    "def get_unet_model(input_shape=(64, 128, 3), num_classes=5, seed=None):\n",
    "    tf.random.set_seed(seed)\n",
    "    input_layer = Input(shape=input_shape, name='input_layer')\n",
    "\n",
    "    # Downsampling path\n",
    "    down_block_1 = unet_block_with_bn(input_layer, 32, name='down_block1_')\n",
    "    d1 = MaxPooling2D()(down_block_1)\n",
    "\n",
    "    down_block_2 = unet_block_with_bn(d1, 64, name='down_block2_')\n",
    "    d2 = MaxPooling2D()(down_block_2)\n",
    "\n",
    "    down_block_3 = unet_block_with_bn(d2, 128, name='down_block3_')\n",
    "    d3 = MaxPooling2D()(down_block_3)\n",
    "\n",
    "    # Bottleneck\n",
    "    bottleneck = bottleneck_with_attention(d3, 256, name='bottleneck')\n",
    "\n",
    "    # Upsampling path\n",
    "    u1_seg = UpSampling2D()(bottleneck)\n",
    "    adjusted_skip1 = conv1x1(down_block_3, 128, name='adjust_skip1')\n",
    "    u1_seg = dynamic_weighted_fusion(adjusted_skip1, u1_seg, name='fusion1')\n",
    "    u1_seg = unet_block_with_ln(u1_seg, 128, name='up_block1_seg')\n",
    "\n",
    "    u2_seg = UpSampling2D()(u1_seg)\n",
    "    adjusted_skip2 = conv1x1(down_block_2, 64, name='adjust_skip2')\n",
    "    u2_seg = dynamic_weighted_fusion(adjusted_skip2, u2_seg, name='fusion2')\n",
    "    u2_seg = unet_block_with_ln(u2_seg, 64, name='up_block2_seg')\n",
    "\n",
    "    u3_seg = UpSampling2D()(u2_seg)\n",
    "    adjusted_skip3 = conv1x1(down_block_1, 32, name='adjust_skip3')\n",
    "    u3_seg = dynamic_weighted_fusion(adjusted_skip3, u3_seg, name='fusion3')\n",
    "    u3_seg = unet_block_with_ln(u3_seg, 32, name='up_block3_seg')\n",
    "\n",
    "    # Output\n",
    "    seg_output = Conv2D(num_classes, kernel_size=1, padding='same', activation=\"softmax\", kernel_initializer=initializer, name='seg_output')(u3_seg)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=seg_output, name='UNet_with_Dynamic_Fusion')\n",
    "    return model\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T18:50:00.942526Z",
     "iopub.status.busy": "2024-12-12T18:50:00.942207Z",
     "iopub.status.idle": "2024-12-12T18:50:00.950521Z",
     "shell.execute_reply": "2024-12-12T18:50:00.949598Z",
     "shell.execute_reply.started": "2024-12-12T18:50:00.942500Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Full U-Net Model\n",
    "def get_unet_model(input_shape=(64, 128, 3), num_classes=5, seed=None):\n",
    "    tf.random.set_seed(seed)\n",
    "    input_layer = Input(shape=input_shape, name='input_layer')\n",
    "\n",
    "    # Downsampling path\n",
    "    down_block_1 = unet_block_with_bn(input_layer, 32, name='down_block1_')\n",
    "    d1 = MaxPooling2D()(down_block_1)\n",
    "\n",
    "    down_block_2 = unet_block_with_bn(d1, 64, name='down_block2_')\n",
    "    d2 = MaxPooling2D()(down_block_2)\n",
    "\n",
    "    down_block_3 = unet_block_with_bn(d2, 128, name='down_block3_')\n",
    "    d3 = MaxPooling2D()(down_block_3)\n",
    "\n",
    "    # Bottleneck\n",
    "    bottleneck = bottleneck_with_attention(d3, 256, name='bottleneck')\n",
    "\n",
    "    # Upsampling path\n",
    "    u1_seg = UpSampling2D()(bottleneck)\n",
    "    adjusted_skip1 = conv1x1(down_block_3, 128, name='adjust_skip1')\n",
    "    u1_seg = dynamic_weighted_fusion(adjusted_skip1, u1_seg, name='fusion1')\n",
    "    u1_seg = unet_block_with_ln(u1_seg, 128, name='up_block1_seg')\n",
    "\n",
    "    u2_seg = UpSampling2D()(u1_seg)\n",
    "    adjusted_skip2 = conv1x1(down_block_2, 64, name='adjust_skip2')\n",
    "    u2_seg = dynamic_weighted_fusion(adjusted_skip2, u2_seg, name='fusion2')\n",
    "    u2_seg = unet_block_with_ln(u2_seg, 64, name='up_block2_seg')\n",
    "\n",
    "    u3_seg = UpSampling2D()(u2_seg)\n",
    "    adjusted_skip3 = conv1x1(down_block_1, 32, name='adjust_skip3')\n",
    "    u3_seg = dynamic_weighted_fusion(adjusted_skip3, u3_seg, name='fusion3')\n",
    "    u3_seg = unet_block_with_ln(u3_seg, 32, name='up_block3_seg')\n",
    "\n",
    "    # Output\n",
    "    seg_output = Conv2D(num_classes, kernel_size=1, padding='same', activation=\"softmax\", kernel_initializer=initializer, name='seg_output')(u3_seg)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=seg_output, name='UNet_with_Dynamic_Fusion')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def focal_loss(alpha=0.25, gamma=2.0, class_weights=None, class_indexes=[1, 2, 3, 4]):\n",
    "    def loss(y_true, y_pred):\n",
    "        # Remove redundant channel dimension if present\n",
    "        y_true = tf.squeeze(y_true, axis=-1)\n",
    "\n",
    "        # Convert y_true to one-hot encoding\n",
    "        num_classes = tf.shape(y_pred)[-1]\n",
    "        y_true_one_hot = tf.one_hot(tf.cast(y_true, tf.int32), depth=num_classes)\n",
    "\n",
    "        # Clip predictions to avoid log(0)\n",
    "        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "\n",
    "        # Filter out unwanted classes (exclude background)\n",
    "        y_true_one_hot = tf.gather(y_true_one_hot, class_indexes, axis=-1)\n",
    "        y_pred = tf.gather(y_pred, class_indexes, axis=-1)\n",
    "\n",
    "        # Compute cross-entropy loss\n",
    "        cross_entropy = -y_true_one_hot * tf.math.log(y_pred)\n",
    "\n",
    "        # Compute the modulating factor (focal loss term)\n",
    "        weights = alpha * tf.math.pow(1 - y_pred, gamma)\n",
    "\n",
    "        # Apply class-specific weights if provided\n",
    "        if class_weights is not None:\n",
    "            # Create a weight tensor for the specified class indexes\n",
    "            class_weight_tensor = tf.constant(\n",
    "                [class_weights.get(cls, 1.0) for cls in class_indexes],\n",
    "                dtype=tf.float32\n",
    "            )\n",
    "            # Gather class weights for each pixel\n",
    "            pixel_weights = tf.gather(class_weight_tensor, tf.argmax(y_true_one_hot, axis=-1))\n",
    "            # Expand pixel_weights to match the last dimension of weights\n",
    "            pixel_weights = tf.expand_dims(pixel_weights, axis=-1)\n",
    "            # Broadcast pixel_weights to all classes\n",
    "            pixel_weights = tf.tile(pixel_weights, [1, 1, 1, len(class_indexes)])\n",
    "            weights = weights * pixel_weights\n",
    "\n",
    "        # Compute the focal loss\n",
    "        focal_loss = tf.reduce_sum(weights * cross_entropy, axis=-1)\n",
    "\n",
    "        return tf.reduce_mean(focal_loss)\n",
    "    return loss\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T18:39:22.089687Z",
     "iopub.status.busy": "2024-12-12T18:39:22.089313Z",
     "iopub.status.idle": "2024-12-12T18:39:22.101234Z",
     "shell.execute_reply": "2024-12-12T18:39:22.100497Z",
     "shell.execute_reply.started": "2024-12-12T18:39:22.089654Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# focal loss(class weights)\n",
    "def focal_loss(alpha=0.25, gamma_dict=None, class_weights=None, class_indexes=[1, 2, 3, 4], label_smoothing=0.1):\n",
    "    def loss(y_true, y_pred):\n",
    "        # Remove redundant channel dimension if present\n",
    "        y_true = tf.squeeze(y_true, axis=-1)\n",
    "\n",
    "        # Convert y_true to one-hot encoding\n",
    "        num_classes = tf.shape(y_pred)[-1]\n",
    "        y_true_one_hot = tf.one_hot(tf.cast(y_true, tf.int32), depth=num_classes)\n",
    "\n",
    "        # Ensure label smoothing and num_classes are float types\n",
    "        y_true_one_hot = tf.cast(y_true_one_hot, tf.float32)\n",
    "        num_classes = tf.cast(num_classes, tf.float32)\n",
    "\n",
    "        # Apply label smoothing\n",
    "        if label_smoothing > 0:\n",
    "            y_true_one_hot = y_true_one_hot * (1 - label_smoothing) + label_smoothing / num_classes\n",
    "\n",
    "        # Clip predictions to avoid log(0)\n",
    "        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "\n",
    "        # Filter out unwanted classes\n",
    "        y_true_one_hot = tf.gather(y_true_one_hot, class_indexes, axis=-1)\n",
    "        y_pred = tf.gather(y_pred, class_indexes, axis=-1)\n",
    "\n",
    "        # Compute cross-entropy loss\n",
    "        cross_entropy = -y_true_one_hot * tf.math.log(y_pred)\n",
    "\n",
    "        # Compute modulating factor with dynamic gamma\n",
    "        if gamma_dict:\n",
    "            gamma_tensor = tf.constant(\n",
    "                [gamma_dict.get(cls, 2.0) for cls in class_indexes],\n",
    "                dtype=tf.float32\n",
    "            )\n",
    "            pixel_gamma = tf.gather(gamma_tensor, tf.argmax(y_true_one_hot, axis=-1))\n",
    "            pixel_gamma = tf.expand_dims(pixel_gamma, axis=-1)  # Expand to [?, 64, 128, 1]\n",
    "            weights = alpha * tf.math.pow(1 - y_pred, pixel_gamma)  # Broadcast to [?, 64, 128, 4]\n",
    "        else:\n",
    "            weights = alpha * tf.math.pow(1 - y_pred, 2.0)\n",
    "\n",
    "        # Apply class-specific weights\n",
    "        if class_weights:\n",
    "            class_weight_tensor = tf.constant(\n",
    "                [class_weights.get(cls, 1.0) for cls in class_indexes],\n",
    "                dtype=tf.float32\n",
    "            )\n",
    "            pixel_weights = tf.gather(class_weight_tensor, tf.argmax(y_true_one_hot, axis=-1))\n",
    "            pixel_weights = tf.expand_dims(pixel_weights, axis=-1)\n",
    "            pixel_weights = tf.tile(pixel_weights, [1, 1, 1, len(class_indexes)])\n",
    "            weights = weights * pixel_weights\n",
    "\n",
    "        # Compute focal loss\n",
    "        focal_loss = tf.reduce_sum(weights * cross_entropy, axis=-1)\n",
    "\n",
    "        # Normalize loss by total foreground pixels\n",
    "        normalizer = tf.reduce_sum(y_true_one_hot, axis=[0, 1, 2])\n",
    "        class_normalized_loss = tf.reduce_sum(focal_loss) / (tf.reduce_sum(normalizer) + tf.keras.backend.epsilon())\n",
    "\n",
    "        return class_normalized_loss\n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T18:39:25.726507Z",
     "iopub.status.busy": "2024-12-12T18:39:25.725685Z",
     "iopub.status.idle": "2024-12-12T18:39:25.741049Z",
     "shell.execute_reply": "2024-12-12T18:39:25.740091Z",
     "shell.execute_reply.started": "2024-12-12T18:39:25.726464Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Combined loss \n",
    "\n",
    "def combined_loss(\n",
    "    alpha=0.25,\n",
    "    gamma_dict=None,\n",
    "    class_weights=None,\n",
    "    class_indexes=[1, 2, 3, 4],\n",
    "    label_smoothing=0.1,\n",
    "    dice_weight=0.5,\n",
    "    focal_weight=0.5,\n",
    "    dice_smooth=1e-6\n",
    "):\n",
    "    \"\"\"\n",
    "    Combined loss function of Focal Loss and Class-Balanced Dice Loss.\n",
    "    \"\"\"\n",
    "    def focal_loss(y_true, y_pred):\n",
    "        # Remove redundant channel dimension if present\n",
    "        y_true = tf.squeeze(y_true, axis=-1)\n",
    "\n",
    "        # Convert y_true to one-hot encoding\n",
    "        num_classes = tf.shape(y_pred)[-1]\n",
    "        y_true_one_hot = tf.one_hot(tf.cast(y_true, tf.int32), depth=num_classes)\n",
    "\n",
    "        # Ensure label smoothing and num_classes are float types\n",
    "        y_true_one_hot = tf.cast(y_true_one_hot, tf.float32)\n",
    "        num_classes = tf.cast(num_classes, tf.float32)\n",
    "\n",
    "        # Apply label smoothing\n",
    "        if label_smoothing > 0:\n",
    "            y_true_one_hot = y_true_one_hot * (1 - label_smoothing) + label_smoothing / num_classes\n",
    "\n",
    "        # Clip predictions to avoid log(0)\n",
    "        y_pred = tf.clip_by_value(y_pred, tf.keras.backend.epsilon(), 1 - tf.keras.backend.epsilon())\n",
    "\n",
    "        # Filter out unwanted classes\n",
    "        y_true_one_hot = tf.gather(y_true_one_hot, class_indexes, axis=-1)\n",
    "        y_pred = tf.gather(y_pred, class_indexes, axis=-1)\n",
    "\n",
    "        # Compute cross-entropy loss\n",
    "        cross_entropy = -y_true_one_hot * tf.math.log(y_pred)\n",
    "\n",
    "        # Compute modulating factor with dynamic gamma\n",
    "        if gamma_dict:\n",
    "            gamma_tensor = tf.constant(\n",
    "                [gamma_dict.get(cls, 2.0) for cls in class_indexes],\n",
    "                dtype=tf.float32\n",
    "            )\n",
    "            pixel_gamma = tf.gather(gamma_tensor, tf.argmax(y_true_one_hot, axis=-1))\n",
    "            pixel_gamma = tf.expand_dims(pixel_gamma, axis=-1)  # Expand to [?, 64, 128, 1]\n",
    "            weights = alpha * tf.math.pow(1 - y_pred, pixel_gamma)  # Broadcast to [?, 64, 128, 4]\n",
    "        else:\n",
    "            weights = alpha * tf.math.pow(1 - y_pred, 2.0)\n",
    "\n",
    "        # Apply class-specific weights\n",
    "        if class_weights:\n",
    "            class_weight_tensor = tf.constant(\n",
    "                [class_weights.get(cls, 1.0) for cls in class_indexes],\n",
    "                dtype=tf.float32\n",
    "            )\n",
    "            pixel_weights = tf.gather(class_weight_tensor, tf.argmax(y_true_one_hot, axis=-1))\n",
    "            pixel_weights = tf.expand_dims(pixel_weights, axis=-1)\n",
    "            pixel_weights = tf.tile(pixel_weights, [1, 1, 1, len(class_indexes)])\n",
    "            weights = weights * pixel_weights\n",
    "\n",
    "        # Compute focal loss\n",
    "        focal_loss = tf.reduce_sum(weights * cross_entropy, axis=-1)\n",
    "\n",
    "        # Normalize loss by total foreground pixels\n",
    "        normalizer = tf.reduce_sum(y_true_one_hot, axis=[0, 1, 2])\n",
    "        class_normalized_loss = tf.reduce_sum(focal_loss) / (tf.reduce_sum(normalizer) + tf.keras.backend.epsilon())\n",
    "\n",
    "        return class_normalized_loss\n",
    "\n",
    "    def class_balanced_dice_loss(y_true, y_pred):\n",
    "        # Remove redundant channel dimension if present\n",
    "        y_true = tf.squeeze(y_true, axis=-1)\n",
    "\n",
    "        # Convert y_true to one-hot encoding\n",
    "        num_classes = tf.shape(y_pred)[-1]\n",
    "        y_true_one_hot = tf.one_hot(tf.cast(y_true, tf.int32), depth=num_classes)\n",
    "\n",
    "        # Ensure y_true_one_hot and y_pred are float types\n",
    "        y_true_one_hot = tf.cast(y_true_one_hot, tf.float32)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "\n",
    "        # Filter out unwanted classes\n",
    "        y_true_one_hot = tf.gather(y_true_one_hot, class_indexes, axis=-1)\n",
    "        y_pred = tf.gather(y_pred, class_indexes, axis=-1)\n",
    "\n",
    "        # Compute Dice numerator and denominator\n",
    "        intersection = tf.reduce_sum(y_true_one_hot * y_pred, axis=[0, 1, 2])\n",
    "        denominator = tf.reduce_sum(y_true_one_hot + y_pred, axis=[0, 1, 2])\n",
    "\n",
    "        dice = (2.0 * intersection + dice_smooth) / (denominator + dice_smooth)\n",
    "\n",
    "        # Apply class weights\n",
    "        if class_weights:\n",
    "            class_weight_tensor = tf.constant(\n",
    "                [class_weights.get(cls, 1.0) for cls in class_indexes],\n",
    "                dtype=tf.float32\n",
    "            )\n",
    "            class_weighted_dice = class_weight_tensor * dice\n",
    "            dice_loss_value = 1.0 - tf.reduce_mean(class_weighted_dice)\n",
    "        else:\n",
    "            dice_loss_value = 1.0 - tf.reduce_mean(dice)\n",
    "\n",
    "        return dice_loss_value\n",
    "\n",
    "    def loss(y_true, y_pred):\n",
    "        # Compute Focal Loss\n",
    "        focal = focal_loss(y_true, y_pred)\n",
    "\n",
    "        # Compute Class-Balanced Dice Loss\n",
    "        dice = class_balanced_dice_loss(y_true, y_pred)\n",
    "\n",
    "        # Combine losses with weights\n",
    "        combined = focal_weight * focal + dice_weight * dice\n",
    "        return combined\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T18:50:10.401764Z",
     "iopub.status.busy": "2024-12-12T18:50:10.401044Z",
     "iopub.status.idle": "2024-12-12T18:50:10.408206Z",
     "shell.execute_reply": "2024-12-12T18:50:10.407210Z",
     "shell.execute_reply.started": "2024-12-12T18:50:10.401727Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define custom Mean Intersection Over Union metric\n",
    "class MeanIntersectionOverUnion(tf.keras.metrics.MeanIoU):\n",
    "    def __init__(self, num_classes, labels_to_exclude=None, name=\"mean_iou\", dtype=None):\n",
    "        super(MeanIntersectionOverUnion, self).__init__(num_classes=num_classes, name=name, dtype=dtype)\n",
    "        if labels_to_exclude is None:\n",
    "            labels_to_exclude = [0]  # Default to excluding label 0\n",
    "        self.labels_to_exclude = labels_to_exclude\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        # Convert predictions to class labels\n",
    "        y_pred = tf.math.argmax(y_pred, axis=-1)\n",
    "        #y_true = tf.math.argmax(y_true,axis=-1) \n",
    "\n",
    "        # Flatten the tensors\n",
    "        y_true = tf.reshape(y_true, [-1])\n",
    "        y_pred = tf.reshape(y_pred, [-1])\n",
    "\n",
    "        # Apply mask to exclude specified labels\n",
    "        for label in self.labels_to_exclude:\n",
    "            mask = tf.not_equal(y_true, label)\n",
    "            y_true = tf.boolean_mask(y_true, mask)\n",
    "            y_pred = tf.boolean_mask(y_pred, mask)\n",
    "\n",
    "        # Update the state\n",
    "        return super().update_state(y_true, y_pred, sample_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T18:50:13.873220Z",
     "iopub.status.busy": "2024-12-12T18:50:13.872505Z",
     "iopub.status.idle": "2024-12-12T18:50:13.880578Z",
     "shell.execute_reply": "2024-12-12T18:50:13.879725Z",
     "shell.execute_reply.started": "2024-12-12T18:50:13.873182Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint\n",
    "\n",
    "# Setup callbacks\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    patience=PATIENCE,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_accuracy',\n",
    "    factor=0.5,\n",
    "    patience=15,\n",
    "    min_lr=1e-7,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath='best_unet.keras',  \n",
    "    monitor='val_mean_iou',       \n",
    "    mode='max',\n",
    "    save_best_only=True,      \n",
    "    verbose=1                 \n",
    ")\n",
    "import numpy as np\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "def cosine_annealing_with_warmup(epoch, lr, warmup_epochs=10, max_lr=1e-3, min_lr=1e-6, total_epochs=100):\n",
    "    if epoch < warmup_epochs:\n",
    "        # Warm-up \n",
    "        return min_lr + (max_lr - min_lr) * (epoch / warmup_epochs)\n",
    "    else:\n",
    "        # Cosine Annealing \n",
    "        cosine_decay = 0.5 * (1 + np.cos(np.pi * (epoch - warmup_epochs) / (total_epochs - warmup_epochs)))\n",
    "        return min_lr + (max_lr - min_lr) * cosine_decay\n",
    "\n",
    "\n",
    "cosine_warmup_scheduler = LearningRateScheduler(\n",
    "    lambda epoch, lr: cosine_annealing_with_warmup(\n",
    "        epoch, \n",
    "        lr, \n",
    "        warmup_epochs=10,   # epochs for warm up\n",
    "        max_lr=1e-3,        # maxmium learning rate\n",
    "        min_lr=1e-6,        # minimum learning rate\n",
    "        total_epochs=EPOCHS   # total training epochs\n",
    "    ),\n",
    "    verbose=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    original_train_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[early_stopping,checkpoint,cosine_warmup_scheduler],\n",
    "    verbose=1\n",
    ").history\n",
    "\n",
    "# Calculate and print the final validation accuracy\n",
    "final_val_meanIoU = round(max(history['val_mean_iou'])* 100, 2)\n",
    "print(f'Final validation Mean Intersection Over Union: {final_val_meanIoU}%')\n",
    "\n",
    "# Save the trained model to a file with the accuracy included in the filename\n",
    "model_filename = 'UNet_'+str(final_val_meanIoU)+'.keras'\n",
    "model.save(model_filename)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T18:50:19.025766Z",
     "iopub.status.busy": "2024-12-12T18:50:19.024918Z",
     "iopub.status.idle": "2024-12-12T19:03:37.257896Z",
     "shell.execute_reply": "2024-12-12T19:03:37.257109Z",
     "shell.execute_reply.started": "2024-12-12T18:50:19.025730Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 1: Training decoder only...\n",
      "\n",
      "Epoch 1: LearningRateScheduler setting learning rate to 1e-06.\n",
      "\n",
      "Epoch 1: val_mean_iou improved from -inf to 0.00767, saving model to best_unet.keras\n",
      "\n",
      "Epoch 2: LearningRateScheduler setting learning rate to 0.00010090000000000001.\n",
      "\n",
      "Epoch 2: val_mean_iou improved from 0.00767 to 0.19937, saving model to best_unet.keras\n",
      "\n",
      "Epoch 3: LearningRateScheduler setting learning rate to 0.00020080000000000003.\n",
      "\n",
      "Epoch 3: val_mean_iou did not improve from 0.19937\n",
      "\n",
      "Epoch 4: LearningRateScheduler setting learning rate to 0.00030070000000000004.\n",
      "\n",
      "Epoch 4: val_mean_iou did not improve from 0.19937\n",
      "\n",
      "Epoch 5: LearningRateScheduler setting learning rate to 0.0004006000000000001.\n",
      "\n",
      "Epoch 5: val_mean_iou did not improve from 0.19937\n",
      "\n",
      "Epoch 6: LearningRateScheduler setting learning rate to 0.0005005000000000001.\n",
      "\n",
      "Epoch 6: val_mean_iou did not improve from 0.19937\n",
      "\n",
      "Epoch 7: LearningRateScheduler setting learning rate to 0.0006004000000000001.\n",
      "\n",
      "Epoch 7: val_mean_iou did not improve from 0.19937\n",
      "\n",
      "Epoch 8: LearningRateScheduler setting learning rate to 0.0007003000000000001.\n",
      "\n",
      "Epoch 8: val_mean_iou improved from 0.19937 to 0.21977, saving model to best_unet.keras\n",
      "\n",
      "Epoch 9: LearningRateScheduler setting learning rate to 0.0008002000000000002.\n",
      "\n",
      "Epoch 9: val_mean_iou improved from 0.21977 to 0.22112, saving model to best_unet.keras\n",
      "\n",
      "Epoch 10: LearningRateScheduler setting learning rate to 0.0009001000000000001.\n",
      "\n",
      "Epoch 10: val_mean_iou improved from 0.22112 to 0.24018, saving model to best_unet.keras\n",
      "\n",
      "Epoch 11: LearningRateScheduler setting learning rate to 0.001.\n",
      "\n",
      "Epoch 11: val_mean_iou did not improve from 0.24018\n",
      "\n",
      "Epoch 12: LearningRateScheduler setting learning rate to 0.000999989733755704.\n",
      "\n",
      "Epoch 12: val_mean_iou did not improve from 0.24018\n",
      "\n",
      "Epoch 13: LearningRateScheduler setting learning rate to 0.0009999589354448206.\n",
      "\n",
      "Epoch 13: val_mean_iou did not improve from 0.24018\n",
      "\n",
      "Epoch 14: LearningRateScheduler setting learning rate to 0.0009999076063333479.\n",
      "\n",
      "Epoch 14: val_mean_iou did not improve from 0.24018\n",
      "\n",
      "Epoch 15: LearningRateScheduler setting learning rate to 0.0009998357485312246.\n",
      "\n",
      "Epoch 15: val_mean_iou improved from 0.24018 to 0.24597, saving model to best_unet.keras\n",
      "\n",
      "Epoch 16: LearningRateScheduler setting learning rate to 0.0009997433649922437.\n",
      "\n",
      "Epoch 16: val_mean_iou improved from 0.24597 to 0.26316, saving model to best_unet.keras\n",
      "\n",
      "Epoch 17: LearningRateScheduler setting learning rate to 0.0009996304595139304.\n",
      "\n",
      "Epoch 17: val_mean_iou did not improve from 0.26316\n",
      "\n",
      "Epoch 18: LearningRateScheduler setting learning rate to 0.0009994970367373868.\n",
      "\n",
      "Epoch 18: val_mean_iou did not improve from 0.26316\n",
      "\n",
      "Epoch 19: LearningRateScheduler setting learning rate to 0.0009993431021471004.\n",
      "\n",
      "Epoch 19: val_mean_iou did not improve from 0.26316\n",
      "\n",
      "Epoch 20: LearningRateScheduler setting learning rate to 0.0009991686620707198.\n",
      "\n",
      "Epoch 20: val_mean_iou did not improve from 0.26316\n",
      "\n",
      "Epoch 21: LearningRateScheduler setting learning rate to 0.0009989737236787931.\n",
      "\n",
      "Epoch 21: val_mean_iou improved from 0.26316 to 0.29176, saving model to best_unet.keras\n",
      "\n",
      "Epoch 22: LearningRateScheduler setting learning rate to 0.000998758294984474.\n",
      "\n",
      "Epoch 22: val_mean_iou did not improve from 0.29176\n",
      "\n",
      "Epoch 23: LearningRateScheduler setting learning rate to 0.000998522384843192.\n",
      "\n",
      "Epoch 23: val_mean_iou did not improve from 0.29176\n",
      "\n",
      "Epoch 24: LearningRateScheduler setting learning rate to 0.0009982660029522897.\n",
      "\n",
      "Epoch 24: val_mean_iou did not improve from 0.29176\n",
      "\n",
      "Epoch 25: LearningRateScheduler setting learning rate to 0.000997989159850622.\n",
      "\n",
      "Epoch 25: val_mean_iou did not improve from 0.29176\n",
      "\n",
      "Epoch 26: LearningRateScheduler setting learning rate to 0.0009976918669181245.\n",
      "\n",
      "Epoch 26: val_mean_iou did not improve from 0.29176\n",
      "\n",
      "Epoch 27: LearningRateScheduler setting learning rate to 0.0009973741363753456.\n",
      "\n",
      "Epoch 27: val_mean_iou improved from 0.29176 to 0.30630, saving model to best_unet.keras\n",
      "\n",
      "Epoch 28: LearningRateScheduler setting learning rate to 0.000997035981282943.\n",
      "\n",
      "Epoch 28: val_mean_iou did not improve from 0.30630\n",
      "\n",
      "Epoch 29: LearningRateScheduler setting learning rate to 0.0009966774155411482.\n",
      "\n",
      "Epoch 29: val_mean_iou improved from 0.30630 to 0.31551, saving model to best_unet.keras\n",
      "\n",
      "Epoch 30: LearningRateScheduler setting learning rate to 0.0009962984538891947.\n",
      "\n",
      "Epoch 30: val_mean_iou did not improve from 0.31551\n",
      "\n",
      "Epoch 31: LearningRateScheduler setting learning rate to 0.0009958991119047114.\n",
      "\n",
      "Epoch 31: val_mean_iou did not improve from 0.31551\n",
      "\n",
      "Epoch 32: LearningRateScheduler setting learning rate to 0.0009954794060030834.\n",
      "\n",
      "Epoch 32: val_mean_iou improved from 0.31551 to 0.32434, saving model to best_unet.keras\n",
      "\n",
      "Epoch 33: LearningRateScheduler setting learning rate to 0.0009950393534367765.\n",
      "\n",
      "Epoch 33: val_mean_iou did not improve from 0.32434\n",
      "\n",
      "Epoch 34: LearningRateScheduler setting learning rate to 0.0009945789722946277.\n",
      "\n",
      "Epoch 34: val_mean_iou did not improve from 0.32434\n",
      "\n",
      "Epoch 35: LearningRateScheduler setting learning rate to 0.000994098281501103.\n",
      "\n",
      "Epoch 35: val_mean_iou did not improve from 0.32434\n",
      "\n",
      "Epoch 36: LearningRateScheduler setting learning rate to 0.000993597300815518.\n",
      "\n",
      "Epoch 36: val_mean_iou did not improve from 0.32434\n",
      "\n",
      "Epoch 37: LearningRateScheduler setting learning rate to 0.0009930760508312262.\n",
      "\n",
      "Epoch 37: val_mean_iou did not improve from 0.32434\n",
      "\n",
      "Epoch 38: LearningRateScheduler setting learning rate to 0.0009925345529747736.\n",
      "\n",
      "Epoch 38: val_mean_iou did not improve from 0.32434\n",
      "\n",
      "Epoch 39: LearningRateScheduler setting learning rate to 0.0009919728295050155.\n",
      "\n",
      "Epoch 39: val_mean_iou did not improve from 0.32434\n",
      "\n",
      "Epoch 40: LearningRateScheduler setting learning rate to 0.0009913909035122038.\n",
      "\n",
      "Epoch 40: val_mean_iou did not improve from 0.32434\n",
      "Phase 2: Training entire model...\n",
      "\n",
      "Epoch 41: LearningRateScheduler setting learning rate to 0.0009907887989170372.\n",
      "\n",
      "Epoch 41: val_mean_iou did not improve from 0.32434\n",
      "\n",
      "Epoch 42: LearningRateScheduler setting learning rate to 0.0009901665404696768.\n",
      "\n",
      "Epoch 42: val_mean_iou did not improve from 0.32434\n",
      "\n",
      "Epoch 43: LearningRateScheduler setting learning rate to 0.0009895241537487304.\n",
      "\n",
      "Epoch 43: val_mean_iou did not improve from 0.32434\n",
      "\n",
      "Epoch 44: LearningRateScheduler setting learning rate to 0.0009888616651601999.\n",
      "\n",
      "Epoch 44: val_mean_iou did not improve from 0.32434\n",
      "\n",
      "Epoch 45: LearningRateScheduler setting learning rate to 0.0009881791019363964.\n",
      "\n",
      "Epoch 45: val_mean_iou did not improve from 0.32434\n",
      "\n",
      "Epoch 46: LearningRateScheduler setting learning rate to 0.000987476492134821.\n",
      "\n",
      "Epoch 46: val_mean_iou did not improve from 0.32434\n",
      "\n",
      "Epoch 47: LearningRateScheduler setting learning rate to 0.0009867538646370101.\n",
      "\n",
      "Epoch 47: val_mean_iou did not improve from 0.32434\n",
      "\n",
      "Epoch 48: LearningRateScheduler setting learning rate to 0.0009860112491473503.\n",
      "\n",
      "Epoch 48: val_mean_iou improved from 0.32434 to 0.35584, saving model to best_unet.keras\n",
      "\n",
      "Epoch 49: LearningRateScheduler setting learning rate to 0.0009852486761918555.\n",
      "\n",
      "Epoch 49: val_mean_iou did not improve from 0.35584\n",
      "\n",
      "Epoch 50: LearningRateScheduler setting learning rate to 0.000984466177116913.\n",
      "\n",
      "Epoch 50: val_mean_iou improved from 0.35584 to 0.36704, saving model to best_unet.keras\n",
      "\n",
      "Epoch 51: LearningRateScheduler setting learning rate to 0.0009836637840879951.\n",
      "\n",
      "Epoch 51: val_mean_iou improved from 0.36704 to 0.39825, saving model to best_unet.keras\n",
      "\n",
      "Epoch 52: LearningRateScheduler setting learning rate to 0.0009828415300883366.\n",
      "\n",
      "Epoch 52: val_mean_iou did not improve from 0.39825\n",
      "\n",
      "Epoch 53: LearningRateScheduler setting learning rate to 0.0009819994489175788.\n",
      "\n",
      "Epoch 53: val_mean_iou did not improve from 0.39825\n",
      "\n",
      "Epoch 54: LearningRateScheduler setting learning rate to 0.0009811375751903801.\n",
      "\n",
      "Epoch 54: val_mean_iou improved from 0.39825 to 0.42485, saving model to best_unet.keras\n",
      "\n",
      "Epoch 55: LearningRateScheduler setting learning rate to 0.000980255944334994.\n",
      "\n",
      "Epoch 55: val_mean_iou did not improve from 0.42485\n",
      "\n",
      "Epoch 56: LearningRateScheduler setting learning rate to 0.000979354592591812.\n",
      "\n",
      "Epoch 56: val_mean_iou did not improve from 0.42485\n",
      "\n",
      "Epoch 57: LearningRateScheduler setting learning rate to 0.0009784335570118736.\n",
      "\n",
      "Epoch 57: val_mean_iou did not improve from 0.42485\n",
      "\n",
      "Epoch 58: LearningRateScheduler setting learning rate to 0.0009774928754553442.\n",
      "\n",
      "Epoch 58: val_mean_iou improved from 0.42485 to 0.48426, saving model to best_unet.keras\n",
      "\n",
      "Epoch 59: LearningRateScheduler setting learning rate to 0.000976532586589958.\n",
      "\n",
      "Epoch 59: val_mean_iou improved from 0.48426 to 0.49267, saving model to best_unet.keras\n",
      "\n",
      "Epoch 60: LearningRateScheduler setting learning rate to 0.0009755527298894294.\n",
      "\n",
      "Epoch 60: val_mean_iou improved from 0.49267 to 0.49522, saving model to best_unet.keras\n",
      "\n",
      "Epoch 61: LearningRateScheduler setting learning rate to 0.0009745533456318291.\n",
      "\n",
      "Epoch 61: val_mean_iou did not improve from 0.49522\n",
      "\n",
      "Epoch 62: LearningRateScheduler setting learning rate to 0.00097353447489793.\n",
      "\n",
      "Epoch 62: val_mean_iou improved from 0.49522 to 0.54553, saving model to best_unet.keras\n",
      "\n",
      "Epoch 63: LearningRateScheduler setting learning rate to 0.000972496159569517.\n",
      "\n",
      "Epoch 63: val_mean_iou did not improve from 0.54553\n",
      "\n",
      "Epoch 64: LearningRateScheduler setting learning rate to 0.0009714384423276667.\n",
      "\n",
      "Epoch 64: val_mean_iou did not improve from 0.54553\n",
      "\n",
      "Epoch 65: LearningRateScheduler setting learning rate to 0.0009703613666509921.\n",
      "\n",
      "Epoch 65: val_mean_iou did not improve from 0.54553\n",
      "\n",
      "Epoch 66: LearningRateScheduler setting learning rate to 0.0009692649768138555.\n",
      "\n",
      "Epoch 66: val_mean_iou did not improve from 0.54553\n",
      "\n",
      "Epoch 67: LearningRateScheduler setting learning rate to 0.0009681493178845488.\n",
      "\n",
      "Epoch 67: val_mean_iou did not improve from 0.54553\n",
      "\n",
      "Epoch 68: LearningRateScheduler setting learning rate to 0.0009670144357234411.\n",
      "\n",
      "Epoch 68: val_mean_iou did not improve from 0.54553\n",
      "\n",
      "Epoch 69: LearningRateScheduler setting learning rate to 0.0009658603769810929.\n",
      "\n",
      "Epoch 69: val_mean_iou did not improve from 0.54553\n",
      "\n",
      "Epoch 70: LearningRateScheduler setting learning rate to 0.0009646871890963389.\n",
      "\n",
      "Epoch 70: val_mean_iou did not improve from 0.54553\n",
      "\n",
      "Epoch 71: LearningRateScheduler setting learning rate to 0.000963494920294338.\n",
      "\n",
      "Epoch 71: val_mean_iou did not improve from 0.54553\n",
      "\n",
      "Epoch 72: LearningRateScheduler setting learning rate to 0.0009622836195845909.\n",
      "\n",
      "Epoch 72: val_mean_iou did not improve from 0.54553\n",
      "\n",
      "Epoch 73: LearningRateScheduler setting learning rate to 0.0009610533367589253.\n",
      "\n",
      "Epoch 73: val_mean_iou did not improve from 0.54553\n",
      "\n",
      "Epoch 74: LearningRateScheduler setting learning rate to 0.0009598041223894498.\n",
      "\n",
      "Epoch 74: val_mean_iou improved from 0.54553 to 0.55007, saving model to best_unet.keras\n",
      "\n",
      "Epoch 75: LearningRateScheduler setting learning rate to 0.0009585360278264739.\n",
      "\n",
      "Epoch 75: val_mean_iou did not improve from 0.55007\n",
      "\n",
      "Epoch 76: LearningRateScheduler setting learning rate to 0.0009572491051963984.\n",
      "\n",
      "Epoch 76: val_mean_iou did not improve from 0.55007\n",
      "\n",
      "Epoch 77: LearningRateScheduler setting learning rate to 0.0009559434073995723.\n",
      "\n",
      "Epoch 77: val_mean_iou did not improve from 0.55007\n",
      "\n",
      "Epoch 78: LearningRateScheduler setting learning rate to 0.0009546189881081177.\n",
      "\n",
      "Epoch 78: val_mean_iou did not improve from 0.55007\n",
      "\n",
      "Epoch 79: LearningRateScheduler setting learning rate to 0.0009532759017637245.\n",
      "\n",
      "Epoch 79: val_mean_iou did not improve from 0.55007\n",
      "\n",
      "Epoch 80: LearningRateScheduler setting learning rate to 0.0009519142035754117.\n",
      "\n",
      "Epoch 80: val_mean_iou did not improve from 0.55007\n",
      "\n",
      "Epoch 81: LearningRateScheduler setting learning rate to 0.0009505339495172585.\n",
      "\n",
      "Epoch 81: val_mean_iou improved from 0.55007 to 0.58206, saving model to best_unet.keras\n",
      "\n",
      "Epoch 82: LearningRateScheduler setting learning rate to 0.0009491351963261029.\n",
      "\n",
      "Epoch 82: val_mean_iou did not improve from 0.58206\n",
      "\n",
      "Epoch 83: LearningRateScheduler setting learning rate to 0.0009477180014992103.\n",
      "\n",
      "Epoch 83: val_mean_iou improved from 0.58206 to 0.60628, saving model to best_unet.keras\n",
      "\n",
      "Epoch 84: LearningRateScheduler setting learning rate to 0.0009462824232919092.\n",
      "\n",
      "Epoch 84: val_mean_iou did not improve from 0.60628\n",
      "\n",
      "Epoch 85: LearningRateScheduler setting learning rate to 0.0009448285207151969.\n",
      "\n",
      "Epoch 85: val_mean_iou did not improve from 0.60628\n",
      "\n",
      "Epoch 86: LearningRateScheduler setting learning rate to 0.0009433563535333137.\n",
      "\n",
      "Epoch 86: val_mean_iou did not improve from 0.60628\n",
      "\n",
      "Epoch 87: LearningRateScheduler setting learning rate to 0.0009418659822612864.\n",
      "\n",
      "Epoch 87: val_mean_iou did not improve from 0.60628\n",
      "\n",
      "Epoch 88: LearningRateScheduler setting learning rate to 0.0009403574681624407.\n",
      "\n",
      "Epoch 88: val_mean_iou did not improve from 0.60628\n",
      "\n",
      "Epoch 89: LearningRateScheduler setting learning rate to 0.0009388308732458829.\n",
      "\n",
      "Epoch 89: val_mean_iou did not improve from 0.60628\n",
      "\n",
      "Epoch 90: LearningRateScheduler setting learning rate to 0.0009372862602639502.\n",
      "\n",
      "Epoch 90: val_mean_iou did not improve from 0.60628\n",
      "\n",
      "Epoch 91: LearningRateScheduler setting learning rate to 0.0009357236927096331.\n",
      "\n",
      "Epoch 91: val_mean_iou did not improve from 0.60628\n",
      "\n",
      "Epoch 92: LearningRateScheduler setting learning rate to 0.0009341432348139633.\n",
      "\n",
      "Epoch 92: val_mean_iou did not improve from 0.60628\n",
      "\n",
      "Epoch 93: LearningRateScheduler setting learning rate to 0.0009325449515433746.\n",
      "\n",
      "Epoch 93: val_mean_iou did not improve from 0.60628\n",
      "\n",
      "Epoch 94: LearningRateScheduler setting learning rate to 0.0009309289085970323.\n",
      "\n",
      "Epoch 94: val_mean_iou did not improve from 0.60628\n",
      "\n",
      "Epoch 95: LearningRateScheduler setting learning rate to 0.0009292951724041322.\n",
      "\n",
      "Epoch 95: val_mean_iou did not improve from 0.60628\n",
      "\n",
      "Epoch 96: LearningRateScheduler setting learning rate to 0.0009276438101211706.\n",
      "\n",
      "Epoch 96: val_mean_iou did not improve from 0.60628\n",
      "\n",
      "Epoch 97: LearningRateScheduler setting learning rate to 0.0009259748896291827.\n",
      "\n",
      "Epoch 97: val_mean_iou did not improve from 0.60628\n",
      "\n",
      "Epoch 98: LearningRateScheduler setting learning rate to 0.0009242884795309532.\n",
      "\n",
      "Epoch 98: val_mean_iou did not improve from 0.60628\n",
      "\n",
      "Epoch 99: LearningRateScheduler setting learning rate to 0.0009225846491481963.\n",
      "\n",
      "Epoch 99: val_mean_iou did not improve from 0.60628\n",
      "\n",
      "Epoch 100: LearningRateScheduler setting learning rate to 0.0009208634685187052.\n",
      "\n",
      "Epoch 100: val_mean_iou improved from 0.60628 to 0.60920, saving model to best_unet.keras\n",
      "\n",
      "Epoch 101: LearningRateScheduler setting learning rate to 0.0009191250083934744.\n",
      "\n",
      "Epoch 101: val_mean_iou improved from 0.60920 to 0.64016, saving model to best_unet.keras\n",
      "\n",
      "Epoch 102: LearningRateScheduler setting learning rate to 0.0009173693402337908.\n",
      "\n",
      "Epoch 102: val_mean_iou did not improve from 0.64016\n",
      "\n",
      "Epoch 103: LearningRateScheduler setting learning rate to 0.0009155965362082955.\n",
      "\n",
      "Epoch 103: val_mean_iou did not improve from 0.64016\n",
      "\n",
      "Epoch 104: LearningRateScheduler setting learning rate to 0.0009138066691900188.\n",
      "\n",
      "Epoch 104: val_mean_iou did not improve from 0.64016\n",
      "\n",
      "Epoch 105: LearningRateScheduler setting learning rate to 0.000911999812753383.\n",
      "\n",
      "Epoch 105: val_mean_iou did not improve from 0.64016\n",
      "\n",
      "Epoch 106: LearningRateScheduler setting learning rate to 0.0009101760411711796.\n",
      "\n",
      "Epoch 106: val_mean_iou did not improve from 0.64016\n",
      "\n",
      "Epoch 107: LearningRateScheduler setting learning rate to 0.0009083354294115148.\n",
      "\n",
      "Epoch 107: val_mean_iou did not improve from 0.64016\n",
      "\n",
      "Epoch 108: LearningRateScheduler setting learning rate to 0.0009064780531347292.\n",
      "\n",
      "Epoch 108: val_mean_iou did not improve from 0.64016\n",
      "\n",
      "Epoch 109: LearningRateScheduler setting learning rate to 0.0009046039886902864.\n",
      "\n",
      "Epoch 109: val_mean_iou did not improve from 0.64016\n",
      "\n",
      "Epoch 110: LearningRateScheduler setting learning rate to 0.0009027133131136355.\n",
      "\n",
      "Epoch 110: val_mean_iou did not improve from 0.64016\n",
      "\n",
      "Epoch 111: LearningRateScheduler setting learning rate to 0.0009008061041230445.\n",
      "\n",
      "Epoch 111: val_mean_iou did not improve from 0.64016\n",
      "\n",
      "Epoch 112: LearningRateScheduler setting learning rate to 0.0008988824401164046.\n",
      "\n",
      "Epoch 112: val_mean_iou did not improve from 0.64016\n",
      "\n",
      "Epoch 113: LearningRateScheduler setting learning rate to 0.0008969424001680089.\n",
      "\n",
      "Epoch 113: val_mean_iou did not improve from 0.64016\n",
      "\n",
      "Epoch 114: LearningRateScheduler setting learning rate to 0.000894986064025301.\n",
      "\n",
      "Epoch 114: val_mean_iou did not improve from 0.64016\n",
      "\n",
      "Epoch 115: LearningRateScheduler setting learning rate to 0.0008930135121055971.\n",
      "\n",
      "Epoch 115: val_mean_iou did not improve from 0.64016\n",
      "\n",
      "Epoch 116: LearningRateScheduler setting learning rate to 0.000891024825492781.\n",
      "\n",
      "Epoch 116: val_mean_iou did not improve from 0.64016\n",
      "\n",
      "Epoch 117: LearningRateScheduler setting learning rate to 0.0008890200859339698.\n",
      "\n",
      "Epoch 117: val_mean_iou did not improve from 0.64016\n",
      "\n",
      "Epoch 118: LearningRateScheduler setting learning rate to 0.0008869993758361552.\n",
      "\n",
      "Epoch 118: val_mean_iou did not improve from 0.64016\n",
      "\n",
      "Epoch 119: LearningRateScheduler setting learning rate to 0.0008849627782628144.\n",
      "\n",
      "Epoch 119: val_mean_iou did not improve from 0.64016\n",
      "\n",
      "Epoch 120: LearningRateScheduler setting learning rate to 0.0008829103769304969.\n",
      "\n",
      "Epoch 120: val_mean_iou did not improve from 0.64016\n",
      "\n",
      "Epoch 121: LearningRateScheduler setting learning rate to 0.0008808422562053827.\n",
      "\n",
      "Epoch 121: val_mean_iou did not improve from 0.64016\n",
      "\n",
      "Epoch 122: LearningRateScheduler setting learning rate to 0.0008787585010998147.\n",
      "\n",
      "Epoch 122: val_mean_iou did not improve from 0.64016\n",
      "\n",
      "Epoch 123: LearningRateScheduler setting learning rate to 0.0008766591972688038.\n",
      "\n",
      "Epoch 123: val_mean_iou did not improve from 0.64016\n",
      "\n",
      "Epoch 124: LearningRateScheduler setting learning rate to 0.0008745444310065078.\n",
      "\n",
      "Epoch 124: val_mean_iou did not improve from 0.64016\n",
      "\n",
      "Epoch 125: LearningRateScheduler setting learning rate to 0.0008724142892426852.\n",
      "\n",
      "Epoch 125: val_mean_iou did not improve from 0.64016\n",
      "\n",
      "Epoch 126: LearningRateScheduler setting learning rate to 0.0008702688595391202.\n",
      "\n",
      "Epoch 126: val_mean_iou did not improve from 0.64016\n",
      "\n",
      "Epoch 127: LearningRateScheduler setting learning rate to 0.0008681082300860253.\n",
      "\n",
      "Epoch 127: val_mean_iou did not improve from 0.64016\n",
      "\n",
      "Epoch 128: LearningRateScheduler setting learning rate to 0.0008659324896984144.\n",
      "\n",
      "Epoch 128: val_mean_iou did not improve from 0.64016\n",
      "\n",
      "Epoch 129: LearningRateScheduler setting learning rate to 0.0008637417278124532.\n",
      "\n",
      "Epoch 129: val_mean_iou did not improve from 0.64016\n",
      "\n",
      "Epoch 130: LearningRateScheduler setting learning rate to 0.0008615360344817822.\n",
      "\n",
      "Epoch 130: val_mean_iou did not improve from 0.64016\n",
      "\n",
      "Epoch 131: LearningRateScheduler setting learning rate to 0.000859315500373815.\n",
      "\n",
      "Epoch 131: val_mean_iou did not improve from 0.64016\n",
      "\n",
      "Epoch 132: LearningRateScheduler setting learning rate to 0.0008570802167660117.\n",
      "\n",
      "Epoch 132: val_mean_iou did not improve from 0.64016\n",
      "\n",
      "Epoch 133: LearningRateScheduler setting learning rate to 0.0008548302755421263.\n",
      "\n",
      "Epoch 133: val_mean_iou did not improve from 0.64016\n",
      "\n",
      "Epoch 134: LearningRateScheduler setting learning rate to 0.0008525657691884302.\n",
      "\n",
      "Epoch 134: val_mean_iou did not improve from 0.64016\n",
      "\n",
      "Epoch 135: LearningRateScheduler setting learning rate to 0.00085028679078991.\n",
      "\n",
      "Epoch 135: val_mean_iou did not improve from 0.64016\n",
      "\n",
      "Epoch 136: LearningRateScheduler setting learning rate to 0.0008479934340264416.\n",
      "\n",
      "Epoch 136: val_mean_iou improved from 0.64016 to 0.65124, saving model to best_unet.keras\n",
      "\n",
      "Epoch 137: LearningRateScheduler setting learning rate to 0.000845685793168939.\n",
      "\n",
      "Epoch 137: val_mean_iou did not improve from 0.65124\n",
      "\n",
      "Epoch 138: LearningRateScheduler setting learning rate to 0.0008433639630754795.\n",
      "\n",
      "Epoch 138: val_mean_iou improved from 0.65124 to 0.65178, saving model to best_unet.keras\n",
      "\n",
      "Epoch 139: LearningRateScheduler setting learning rate to 0.0008410280391874045.\n",
      "\n",
      "Epoch 139: val_mean_iou improved from 0.65178 to 0.66514, saving model to best_unet.keras\n",
      "\n",
      "Epoch 140: LearningRateScheduler setting learning rate to 0.0008386781175253954.\n",
      "\n",
      "Epoch 140: val_mean_iou did not improve from 0.66514\n",
      "\n",
      "Epoch 141: LearningRateScheduler setting learning rate to 0.0008363142946855279.\n",
      "\n",
      "Epoch 141: val_mean_iou did not improve from 0.66514\n",
      "\n",
      "Epoch 142: LearningRateScheduler setting learning rate to 0.0008339366678353006.\n",
      "\n",
      "Epoch 142: val_mean_iou did not improve from 0.66514\n",
      "\n",
      "Epoch 143: LearningRateScheduler setting learning rate to 0.0008315453347096406.\n",
      "\n",
      "Epoch 143: val_mean_iou did not improve from 0.66514\n",
      "\n",
      "Epoch 144: LearningRateScheduler setting learning rate to 0.0008291403936068865.\n",
      "\n",
      "Epoch 144: val_mean_iou did not improve from 0.66514\n",
      "\n",
      "Epoch 145: LearningRateScheduler setting learning rate to 0.0008267219433847479.\n",
      "\n",
      "Epoch 145: val_mean_iou did not improve from 0.66514\n",
      "\n",
      "Epoch 146: LearningRateScheduler setting learning rate to 0.0008242900834562404.\n",
      "\n",
      "Epoch 146: val_mean_iou did not improve from 0.66514\n",
      "\n",
      "Epoch 147: LearningRateScheduler setting learning rate to 0.0008218449137856012.\n",
      "\n",
      "Epoch 147: val_mean_iou improved from 0.66514 to 0.67430, saving model to best_unet.keras\n",
      "\n",
      "Epoch 148: LearningRateScheduler setting learning rate to 0.000819386534884178.\n",
      "\n",
      "Epoch 148: val_mean_iou did not improve from 0.67430\n",
      "\n",
      "Epoch 149: LearningRateScheduler setting learning rate to 0.0008169150478062993.\n",
      "\n",
      "Epoch 149: val_mean_iou did not improve from 0.67430\n",
      "\n",
      "Epoch 150: LearningRateScheduler setting learning rate to 0.0008144305541451178.\n",
      "\n",
      "Epoch 150: val_mean_iou did not improve from 0.67430\n",
      "\n",
      "Epoch 151: LearningRateScheduler setting learning rate to 0.0008119331560284375.\n",
      "\n",
      "Epoch 151: val_mean_iou did not improve from 0.67430\n",
      "\n",
      "Epoch 152: LearningRateScheduler setting learning rate to 0.000809422956114513.\n",
      "\n",
      "Epoch 152: val_mean_iou did not improve from 0.67430\n",
      "\n",
      "Epoch 153: LearningRateScheduler setting learning rate to 0.0008069000575878311.\n",
      "\n",
      "Epoch 153: val_mean_iou did not improve from 0.67430\n",
      "\n",
      "Epoch 154: LearningRateScheduler setting learning rate to 0.000804364564154869.\n",
      "\n",
      "Epoch 154: val_mean_iou did not improve from 0.67430\n",
      "\n",
      "Epoch 155: LearningRateScheduler setting learning rate to 0.0008018165800398302.\n",
      "\n",
      "Epoch 155: val_mean_iou did not improve from 0.67430\n",
      "\n",
      "Epoch 156: LearningRateScheduler setting learning rate to 0.0007992562099803624.\n",
      "\n",
      "Epoch 156: val_mean_iou did not improve from 0.67430\n",
      "\n",
      "Epoch 157: LearningRateScheduler setting learning rate to 0.0007966835592232505.\n",
      "\n",
      "Epoch 157: val_mean_iou did not improve from 0.67430\n",
      "\n",
      "Epoch 158: LearningRateScheduler setting learning rate to 0.0007940987335200904.\n",
      "\n",
      "Epoch 158: val_mean_iou improved from 0.67430 to 0.68488, saving model to best_unet.keras\n",
      "\n",
      "Epoch 159: LearningRateScheduler setting learning rate to 0.0007915018391229429.\n",
      "\n",
      "Epoch 159: val_mean_iou did not improve from 0.68488\n",
      "\n",
      "Epoch 160: LearningRateScheduler setting learning rate to 0.0007888929827799654.\n",
      "\n",
      "Epoch 160: val_mean_iou did not improve from 0.68488\n",
      "\n",
      "Epoch 161: LearningRateScheduler setting learning rate to 0.0007862722717310238.\n",
      "\n",
      "Epoch 161: val_mean_iou did not improve from 0.68488\n",
      "\n",
      "Epoch 162: LearningRateScheduler setting learning rate to 0.0007836398137032849.\n",
      "\n",
      "Epoch 162: val_mean_iou did not improve from 0.68488\n",
      "\n",
      "Epoch 163: LearningRateScheduler setting learning rate to 0.0007809957169067873.\n",
      "\n",
      "Epoch 163: val_mean_iou did not improve from 0.68488\n",
      "\n",
      "Epoch 164: LearningRateScheduler setting learning rate to 0.0007783400900299943.\n",
      "\n",
      "Epoch 164: val_mean_iou improved from 0.68488 to 0.69096, saving model to best_unet.keras\n",
      "\n",
      "Epoch 165: LearningRateScheduler setting learning rate to 0.0007756730422353253.\n",
      "\n",
      "Epoch 165: val_mean_iou did not improve from 0.69096\n",
      "\n",
      "Epoch 166: LearningRateScheduler setting learning rate to 0.0007729946831546693.\n",
      "\n",
      "Epoch 166: val_mean_iou did not improve from 0.69096\n",
      "\n",
      "Epoch 167: LearningRateScheduler setting learning rate to 0.0007703051228848773.\n",
      "\n",
      "Epoch 167: val_mean_iou did not improve from 0.69096\n",
      "\n",
      "Epoch 168: LearningRateScheduler setting learning rate to 0.0007676044719832378.\n",
      "\n",
      "Epoch 168: val_mean_iou did not improve from 0.69096\n",
      "\n",
      "Epoch 169: LearningRateScheduler setting learning rate to 0.000764892841462932.\n",
      "\n",
      "Epoch 169: val_mean_iou did not improve from 0.69096\n",
      "\n",
      "Epoch 170: LearningRateScheduler setting learning rate to 0.0007621703427884692.\n",
      "\n",
      "Epoch 170: val_mean_iou did not improve from 0.69096\n",
      "\n",
      "Epoch 171: LearningRateScheduler setting learning rate to 0.0007594370878711075.\n",
      "\n",
      "Epoch 171: val_mean_iou did not improve from 0.69096\n",
      "\n",
      "Epoch 172: LearningRateScheduler setting learning rate to 0.0007566931890642503.\n",
      "\n",
      "Epoch 172: val_mean_iou improved from 0.69096 to 0.69966, saving model to best_unet.keras\n",
      "\n",
      "Epoch 173: LearningRateScheduler setting learning rate to 0.0007539387591588307.\n",
      "\n",
      "Epoch 173: val_mean_iou improved from 0.69966 to 0.70186, saving model to best_unet.keras\n",
      "\n",
      "Epoch 174: LearningRateScheduler setting learning rate to 0.0007511739113786741.\n",
      "\n",
      "Epoch 174: val_mean_iou did not improve from 0.70186\n",
      "\n",
      "Epoch 175: LearningRateScheduler setting learning rate to 0.0007483987593758431.\n",
      "\n",
      "Epoch 175: val_mean_iou did not improve from 0.70186\n",
      "\n",
      "Epoch 176: LearningRateScheduler setting learning rate to 0.0007456134172259672.\n",
      "\n",
      "Epoch 176: val_mean_iou did not improve from 0.70186\n",
      "\n",
      "Epoch 177: LearningRateScheduler setting learning rate to 0.0007428179994235526.\n",
      "\n",
      "Epoch 177: val_mean_iou did not improve from 0.70186\n",
      "\n",
      "Epoch 178: LearningRateScheduler setting learning rate to 0.0007400126208772764.\n",
      "\n",
      "Epoch 178: val_mean_iou did not improve from 0.70186\n",
      "\n",
      "Epoch 179: LearningRateScheduler setting learning rate to 0.0007371973969052629.\n",
      "\n",
      "Epoch 179: val_mean_iou did not improve from 0.70186\n",
      "\n",
      "Epoch 180: LearningRateScheduler setting learning rate to 0.0007343724432303429.\n",
      "\n",
      "Epoch 180: val_mean_iou did not improve from 0.70186\n",
      "\n",
      "Epoch 181: LearningRateScheduler setting learning rate to 0.0007315378759752974.\n",
      "\n",
      "Epoch 181: val_mean_iou did not improve from 0.70186\n",
      "\n",
      "Epoch 182: LearningRateScheduler setting learning rate to 0.0007286938116580839.\n",
      "\n",
      "Epoch 182: val_mean_iou did not improve from 0.70186\n",
      "\n",
      "Epoch 183: LearningRateScheduler setting learning rate to 0.0007258403671870472.\n",
      "\n",
      "Epoch 183: val_mean_iou did not improve from 0.70186\n",
      "\n",
      "Epoch 184: LearningRateScheduler setting learning rate to 0.0007229776598561134.\n",
      "\n",
      "Epoch 184: val_mean_iou did not improve from 0.70186\n",
      "\n",
      "Epoch 185: LearningRateScheduler setting learning rate to 0.0007201058073399683.\n",
      "\n",
      "Epoch 185: val_mean_iou did not improve from 0.70186\n",
      "\n",
      "Epoch 186: LearningRateScheduler setting learning rate to 0.0007172249276892204.\n",
      "\n",
      "Epoch 186: val_mean_iou did not improve from 0.70186\n",
      "\n",
      "Epoch 187: LearningRateScheduler setting learning rate to 0.0007143351393255489.\n",
      "\n",
      "Epoch 187: val_mean_iou did not improve from 0.70186\n",
      "\n",
      "Epoch 188: LearningRateScheduler setting learning rate to 0.0007114365610368344.\n",
      "\n",
      "Epoch 188: val_mean_iou did not improve from 0.70186\n",
      "\n",
      "Epoch 189: LearningRateScheduler setting learning rate to 0.0007085293119722779.\n",
      "\n",
      "Epoch 189: val_mean_iou did not improve from 0.70186\n",
      "\n",
      "Epoch 190: LearningRateScheduler setting learning rate to 0.0007056135116375011.\n",
      "\n",
      "Epoch 190: val_mean_iou did not improve from 0.70186\n",
      "\n",
      "Epoch 191: LearningRateScheduler setting learning rate to 0.0007026892798896357.\n",
      "\n",
      "Epoch 191: val_mean_iou did not improve from 0.70186\n",
      "\n",
      "Epoch 192: LearningRateScheduler setting learning rate to 0.0006997567369323955.\n",
      "\n",
      "Epoch 192: val_mean_iou did not improve from 0.70186\n",
      "\n",
      "Epoch 193: LearningRateScheduler setting learning rate to 0.000696816003311135.\n",
      "\n",
      "Epoch 193: val_mean_iou did not improve from 0.70186\n",
      "\n",
      "Epoch 194: LearningRateScheduler setting learning rate to 0.0006938671999078956.\n",
      "\n",
      "Epoch 194: val_mean_iou did not improve from 0.70186\n",
      "\n",
      "Epoch 195: LearningRateScheduler setting learning rate to 0.0006909104479364355.\n",
      "\n",
      "Epoch 195: val_mean_iou did not improve from 0.70186\n",
      "\n",
      "Epoch 196: LearningRateScheduler setting learning rate to 0.0006879458689372475.\n",
      "\n",
      "Epoch 196: val_mean_iou did not improve from 0.70186\n",
      "\n",
      "Epoch 197: LearningRateScheduler setting learning rate to 0.0006849735847725628.\n",
      "\n",
      "Epoch 197: val_mean_iou did not improve from 0.70186\n",
      "\n",
      "Epoch 198: LearningRateScheduler setting learning rate to 0.0006819937176213415.\n",
      "\n",
      "Epoch 198: val_mean_iou did not improve from 0.70186\n",
      "\n",
      "Epoch 199: LearningRateScheduler setting learning rate to 0.0006790063899742511.\n",
      "\n",
      "Epoch 199: val_mean_iou did not improve from 0.70186\n",
      "\n",
      "Epoch 200: LearningRateScheduler setting learning rate to 0.0006760117246286308.\n",
      "\n",
      "Epoch 200: val_mean_iou did not improve from 0.70186\n",
      "\n",
      "Epoch 201: LearningRateScheduler setting learning rate to 0.0006730098446834432.\n",
      "\n",
      "Epoch 201: val_mean_iou did not improve from 0.70186\n",
      "\n",
      "Epoch 202: LearningRateScheduler setting learning rate to 0.0006700008735342157.\n",
      "\n",
      "Epoch 202: val_mean_iou did not improve from 0.70186\n",
      "\n",
      "Epoch 203: LearningRateScheduler setting learning rate to 0.0006669849348679666.\n",
      "\n",
      "Epoch 203: val_mean_iou did not improve from 0.70186\n",
      "Final validation Mean Intersection Over Union: 70.19%\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Set learning rate for the optimiser\n",
    "LEARNING_RATE = 1e-3\n",
    "# Set early stopping patience threshold\n",
    "PATIENCE = 30\n",
    "# Set maximum number of training epochs\n",
    "EPOCHS_PHASE1 = 40 \n",
    "EPOCHS_PHASE2 = 360  \n",
    "TOTAL_EPOCHS = EPOCHS_PHASE1 + EPOCHS_PHASE2\n",
    "\n",
    "model = get_unet_model()\n",
    "\n",
    "def calculate_class_weights(y_train):\n",
    "    flattened_labels = np.concatenate([label.flatten() for label in y_train])\n",
    "    class_counts = Counter(flattened_labels)\n",
    "    total_pixels = sum(class_counts.values())\n",
    "    class_weights = {cls: total_pixels / count for cls, count in class_counts.items()}\n",
    "    max_weight = max(class_weights.values())\n",
    "    normalized_class_weights = {cls: weight / max_weight for cls, weight in class_weights.items()}\n",
    "    return normalized_class_weights\n",
    "\n",
    "class_weights = calculate_class_weights(y_train)\n",
    "\n",
    "gamma_dict = {\n",
    "    1: 2.0,\n",
    "    2: 5.0,\n",
    "    3: 2.0,\n",
    "    4: 10.0\n",
    "}\n",
    "\n",
    "# Compile and train the model (Multi-stage training)\n",
    "\n",
    "# Step 1: train only the decoder part\n",
    "print(\"Phase 1: Training decoder only...\")\n",
    "for layer in model.layers:\n",
    "    if 'down_block' in layer.name or 'bottleneck' in layer.name:\n",
    "        layer.trainable = False\n",
    "\n",
    "model.compile(\n",
    "     loss=focal_loss(\n",
    "        alpha=0.25,\n",
    "        gamma_dict=gamma_dict,\n",
    "        class_weights=class_weights,\n",
    "        class_indexes=[1, 2, 3, 4],\n",
    "        label_smoothing=0.1\n",
    "    ),\n",
    "    optimizer=tf.keras.optimizers.AdamW(learning_rate=LEARNING_RATE),\n",
    "    metrics=[\"accuracy\", MeanIntersectionOverUnion(num_classes=5, labels_to_exclude=[0], name=\"mean_iou\")]\n",
    ")\n",
    "\n",
    "history_phase1 = model.fit(\n",
    "    original_train_dataset,\n",
    "    epochs=EPOCHS_PHASE1,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[early_stopping, checkpoint, cosine_warmup_scheduler],\n",
    "    verbose=0\n",
    ").history\n",
    "\n",
    "# Step 2: defrozen all and train the entire model\n",
    "print(\"Phase 2: Training entire model...\")\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "model.compile(\n",
    "     loss=focal_loss(\n",
    "        alpha=0.25,\n",
    "        gamma_dict=gamma_dict,\n",
    "        class_weights=class_weights,\n",
    "        class_indexes=[1, 2, 3, 4],\n",
    "        label_smoothing=0.1\n",
    "    ),\n",
    "    optimizer=tf.keras.optimizers.AdamW(learning_rate=LEARNING_RATE / 10),  \n",
    "    metrics=[\"accuracy\", MeanIntersectionOverUnion(num_classes=5, labels_to_exclude=[0], name=\"mean_iou\")]\n",
    ")\n",
    "\n",
    "history_phase2 = model.fit(\n",
    "    original_train_dataset,\n",
    "    epochs=EPOCHS_PHASE2,\n",
    "    initial_epoch=EPOCHS_PHASE1,  \n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[early_stopping, checkpoint, cosine_warmup_scheduler],\n",
    "    verbose=0\n",
    ").history\n",
    "\n",
    "history = {key: history_phase1[key] + history_phase2[key] for key in history_phase1}\n",
    "\n",
    "# Final validation metrics\n",
    "final_val_meanIoU = round(max(history['val_mean_iou']) * 100, 2)\n",
    "print(f'Final validation Mean Intersection Over Union: {final_val_meanIoU}%')\n",
    "\n",
    "# Save the trained model\n",
    "model_filename = f'UNet_{final_val_meanIoU}.keras'\n",
    "model.save(model_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-12-12T14:57:38.863439Z",
     "iopub.status.busy": "2024-12-12T14:57:38.862621Z",
     "iopub.status.idle": "2024-12-12T14:57:43.197366Z",
     "shell.execute_reply": "2024-12-12T14:57:43.196265Z",
     "shell.execute_reply.started": "2024-12-12T14:57:38.863406Z"
    },
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Class 0 IoU: 0.00%\n",
      "Class 1 IoU: 73.95%\n",
      "Class 2 IoU: 55.97%\n",
      "Class 3 IoU: 64.70%\n",
      "Class 4 IoU: 4.83%\n",
      "Mean IoU: 39.89%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Function to calculate IoU for each class\n",
    "def calculate_classwise_iou(y_true, y_pred, num_classes):\n",
    "    ious = []\n",
    "    for class_id in range(num_classes):\n",
    "        # Create a mask for each class\n",
    "        true_mask = (y_true == class_id)\n",
    "        pred_mask = (y_pred == class_id)\n",
    "        \n",
    "        # Calculate intersection and union\n",
    "        intersection = np.logical_and(true_mask, pred_mask).sum()\n",
    "        union = np.logical_or(true_mask, pred_mask).sum()\n",
    "        \n",
    "        # Calculate IoU and handle division by zero\n",
    "        iou = intersection / union if union > 0 else 0.0\n",
    "        ious.append(iou)\n",
    "    return ious\n",
    "\n",
    "# Predict and calculate classwise IoU\n",
    "def evaluate_classwise_iou(model, val_dataset, num_classes):\n",
    "    all_true = []\n",
    "    all_pred = []\n",
    "    \n",
    "    for images, labels in val_dataset:\n",
    "        # Predict the segmentation masks\n",
    "        predictions = model.predict(images)\n",
    "        predictions = np.argmax(predictions, axis=-1)  # Get the predicted class indices\n",
    "        labels = np.squeeze(labels)  # Remove redundant dimensions\n",
    "        \n",
    "        # Collect predictions and true labels\n",
    "        all_true.append(labels)\n",
    "        all_pred.append(predictions)\n",
    "    \n",
    "    # Stack all batches\n",
    "    all_true = np.concatenate(all_true, axis=0)\n",
    "    all_pred = np.concatenate(all_pred, axis=0)\n",
    "    \n",
    "    # Calculate IoU for each class\n",
    "    classwise_ious = calculate_classwise_iou(all_true, all_pred, num_classes)\n",
    "    return classwise_ious\n",
    "\n",
    "# Example usage after training\n",
    "num_classes = 5  # Change this to match your number of classes\n",
    "classwise_ious = evaluate_classwise_iou(model, val_dataset, num_classes)\n",
    "\n",
    "# Print per-class IoU\n",
    "for i, iou in enumerate(classwise_ious):\n",
    "    print(f\"Class {i} IoU: {iou:.2%}\")\n",
    "\n",
    "# Calculate mean IoU\n",
    "mean_iou = np.mean(classwise_ious)\n",
    "print(f\"Mean IoU: {mean_iou:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T15:18:47.904655Z",
     "iopub.status.busy": "2024-12-12T15:18:47.904270Z",
     "iopub.status.idle": "2024-12-12T15:18:48.972195Z",
     "shell.execute_reply": "2024-12-12T15:18:48.971362Z",
     "shell.execute_reply.started": "2024-12-12T15:18:47.904624Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model.load_weights('/kaggle/working/UNet_67.62.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T19:07:52.324543Z",
     "iopub.status.busy": "2024-12-12T19:07:52.324217Z",
     "iopub.status.idle": "2024-12-12T19:07:52.329455Z",
     "shell.execute_reply": "2024-12-12T19:07:52.328565Z",
     "shell.execute_reply.started": "2024-12-12T19:07:52.324516Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def convert_to_rgb(images):\n",
    "    images_tensor = tf.convert_to_tensor(images, dtype=tf.float32)\n",
    "    \n",
    "    images_tensor = tf.expand_dims(images_tensor, axis=-1)\n",
    "    \n",
    "    images_rgb = tf.image.grayscale_to_rgb(images_tensor)\n",
    "    \n",
    "    return images_rgb.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T19:09:32.610241Z",
     "iopub.status.busy": "2024-12-12T19:09:32.609895Z",
     "iopub.status.idle": "2024-12-12T19:09:49.080460Z",
     "shell.execute_reply": "2024-12-12T19:09:49.079486Z",
     "shell.execute_reply.started": "2024-12-12T19:09:32.610206Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed X_test shape: (10022, 64, 128, 3)\n",
      "\u001b[1m314/314\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 22ms/step\n",
      "Predictions shape: (10022, 64, 128)\n"
     ]
    }
   ],
   "source": [
    "X_test = convert_to_rgb(X_test)\n",
    "\n",
    "X_test = X_test.astype(\"float32\")/255.0\n",
    "print(f\"Fixed X_test shape: {X_test.shape}\")\n",
    "preds = model.predict(X_test)\n",
    "preds = np.argmax(preds, axis=-1)\n",
    "print(f\"Predictions shape: {preds.shape}\")\n",
    "import pandas as pd\n",
    "def y_to_df(y) -> pd.DataFrame:\n",
    "    \"\"\"Converts segmentation predictions into a DataFrame format for Kaggle.\"\"\"\n",
    "    n_samples = len(y)\n",
    "    y_flat = y.reshape(n_samples, -1)\n",
    "    df = pd.DataFrame(y_flat)\n",
    "    df[\"id\"] = np.arange(n_samples)\n",
    "    cols = [\"id\"] + [col for col in df.columns if col != \"id\"]\n",
    "    return df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-12T19:09:54.121859Z",
     "iopub.status.busy": "2024-12-12T19:09:54.121002Z",
     "iopub.status.idle": "2024-12-12T19:10:15.801771Z",
     "shell.execute_reply": "2024-12-12T19:10:15.801051Z",
     "shell.execute_reply.started": "2024-12-12T19:09:54.121811Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create and download the csv submission file\n",
    "timestep_str = model_filename.replace(\"model_\", \"\").replace(\".keras\", \"\")\n",
    "submission_filename = f\"submission_{timestep_str}.csv\"\n",
    "submission_df = y_to_df(preds)\n",
    "submission_df.to_csv(submission_filename, index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6276435,
     "sourceId": 10163907,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
